{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NM vs. GD: Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import numpy.linalg as lnp\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Problem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 [Iris flower data set] (https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)\n",
    "We experiment with Newton's Method and Gradient Descent using the Iris Flower Dataset, provided as a built-in dataset in Sklearn. Iris is a multivariate dataset with 3 inference classes of iris flower -- Setosa, Versicolour, and Virginica, as well as 4 dependent features to guide prediction -- Sepal length, Sepal width, Petal length, Petal width. \n",
    "\n",
    "We utilise multivariate linear regression, optimised by Gradient Descent and Newton's Method respectively to model to relationship between independent variable $y$ (iris flower class) and dependent variables $x=[x_1, x_2, x_3, x_4]$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "data, valid = datasets.load_iris(), True \n",
    "X = data[\"data\"]\n",
    "Y = data[\"target\"]\n",
    "m = Y.shape[0]\n",
    "\n",
    "# print(data[\"feature_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "def MSE(dist):\n",
    "    global m\n",
    "    return lnp.norm(dist**2, 1)/(2*m)\n",
    "\n",
    "# for the bias:\n",
    "X = np.insert(X, 0, 1, axis=1)\n",
    "if valid:\n",
    "    margin = 0.0233 #global minima at 3 sig fig for MSE on this distribution & overflow\n",
    "else:\n",
    "    margin = 11 #preliminary\n",
    "\n",
    "#random init: sample to make better\n",
    "#np.random.seed(1)\n",
    "#theta = np.ndarray((X.shape[1]))\n",
    "theta = np.full((X.shape[1]), 5) #weights\n",
    "y_hat = np.matmul(X, theta)\n",
    "dist = Y-y_hat\n",
    "X_T = X.T\n",
    "cost = MSE(dist) #no need transpose because np doesn't differentiate between column & row vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Newton's Method in Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nm(theta, y_hat, X_T, dist, cost, lr):\n",
    "    start = time()\n",
    "    global X, Y, margin, m, indices\n",
    "    cost_log = []\n",
    "    epoch = 0\n",
    "    hessian = np.matmul(X_T, X)\n",
    "    inv_hessian = lnp.inv(hessian)\n",
    "\n",
    "    while cost > margin:\n",
    "        #cost_log.append(cost) #commented for timing\n",
    "        #print(\"Epoch #\" + str(epoch) + \":\", cost)\n",
    "        epoch += 1\n",
    "        grad = np.matmul(X_T, dist/-m)\n",
    "        theta = theta-lr*np.matmul(inv_hessian, grad)\n",
    "\n",
    "        y_hat = np.matmul(X, theta)\n",
    "        dist = Y-y_hat\n",
    "        X_T = X.T\n",
    "        cost = MSE(dist)\n",
    "    end = time()\n",
    "    cost_log.append(cost)\n",
    "    print(\"\\n\" + \"Finished NM in \" + str(epoch+1), \"epochs with error \" + str(cost_log[-1]) + \"\\n\")\n",
    "    print(\"Optimal theta:\", theta)\n",
    "    print(\"\\n\\n\" + \"y_hat, y\")\n",
    "    for i in indices:\n",
    "        print(y_hat[i], Y[i])\n",
    "    return end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(theta, y_hat, X_T, dist, cost, lr):\n",
    "    start = time()\n",
    "    global X, Y, margin, m, indices\n",
    "    cost_log = []\n",
    "    epoch = 0\n",
    "\n",
    "    while cost > margin:\n",
    "        #cost_log.append(cost) #commented for timing\n",
    "        epoch += 1\n",
    "        grad = np.matmul(X_T, dist)/-m\n",
    "        theta = theta-lr*grad\n",
    "\n",
    "        y_hat = np.matmul(X, theta)\n",
    "        dist = Y-y_hat\n",
    "        X_T = X.T\n",
    "        cost = MSE(dist)\n",
    "    end = time()\n",
    "    cost_log.append(cost)\n",
    "    print(\"\\n\" + \"Finished GD in \" + str(epoch+1), \"epochs with error \" + str(cost_log[-1]) + \"\\n\")\n",
    "    print(\"Optimal theta:\", theta)\n",
    "\n",
    "    print(\"\\n\\n\" + \"y_hat, y\")\n",
    "    for i in indices:\n",
    "        print(y_hat[i], Y[i])\n",
    "    return end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sampling & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished GD in 13430 epochs with error 0.023299961074107964\n",
      "\n",
      "Optimal theta: [ 0.34672323 -0.13559269 -0.05407266  0.23162183  0.61839075]\n",
      "\n",
      "\n",
      "y_hat, y\n",
      "1.1265075222822922 1\n",
      "-0.061731272562303555 0\n",
      "0.9271354666962658 1\n",
      "0.9352874697216145 1\n",
      "1.3950705658803275 1\n",
      "-0.08023001233992957 0\n",
      "-0.11443522828992857 0\n",
      "1.2508213455115704 1\n",
      "0.09881462220891538 0\n",
      "1.419840745672771 1\n",
      "0.0225261632191371 0\n",
      "1.2508213455115704 1\n",
      "1.770503224977595 2\n",
      "1.4854076742442344 1\n",
      "-0.038808383256667925 0\n",
      "0.02648251780110357 0\n",
      "-0.16350433180021967 0\n",
      "0.006304365671656667 0\n",
      "1.2757818831859362 1\n",
      "0.048937612160696115 0\n",
      "0.012683955065178956 0\n",
      "1.221579489762053 1\n",
      "1.7458641896678622 2\n",
      "1.5410697201339756 2\n",
      "-0.10087595941541959 0\n",
      "0.048937612160696115 0\n",
      "-0.12623287630939942 0\n",
      "-0.024265998061008487 0\n",
      "-0.0020869312151078484 0\n",
      "0.04992072848184678 0\n",
      "1.286835708745672 1\n",
      "-0.08610507313513273 0\n",
      "0.9465805891427661 1\n",
      "0.04475275649410215 0\n",
      "-0.10264837304724596 0\n",
      "1.7002577049180065 2\n",
      "1.640699927395939 2\n",
      "-0.09425707616048155 0\n",
      "1.28859732960071 1\n",
      "1.178911561198861 1\n",
      "2.134664350207287 2\n",
      "-0.06928574570932775 0\n",
      "1.2647172648494118 1\n",
      "-0.05188906440834566 0\n",
      "1.7595805439005694 2\n",
      "1.1787436830638742 1\n",
      "0.8951141916964072 1\n",
      "1.4221891036293455 1\n",
      "1.0683400330777781 1\n",
      "-0.07603436389654741 0\n",
      "1.7724067830921317 2\n",
      "1.1911662113875867 1\n",
      "1.1265075222822922 1\n",
      "1.7458641896678622 2\n",
      "1.6054401793983613 2\n",
      "0.9823297176399639 1\n",
      "1.923229411864886 2\n",
      "-0.014195288822423144 0\n",
      "1.2647172648494118 1\n",
      "-0.06928574570932775 0\n",
      "-0.12623287630939942 0\n",
      "0.030678166244485813 0\n",
      "0.09881462220891538 0\n",
      "-0.10087595941541959 0\n",
      "0.006304365671656667 0\n",
      "1.1988958943178334 1\n",
      "2.007019052700284 2\n",
      "-0.017325612441437627 0\n",
      "1.8869865475461571 2\n",
      "1.7724067830921317 2\n",
      "1.8470785042578532 2\n",
      "1.2069916297154528 1\n",
      "1.05968350145411 1\n",
      "0.030678166244485813 0\n",
      "1.346240756231452 1\n",
      "1.218127663778406 1\n",
      "-0.17078482901167555 0\n",
      "-0.08610507313513273 0\n",
      "1.4854076742442344 1\n",
      "-0.06928574570932775 0\n",
      "1.05968350145411 1\n",
      "0.006304365671656667 0\n",
      "1.0683400330777781 1\n",
      "0.09881462220891538 0\n",
      "-0.014195288822423144 0\n",
      "1.0985996872225987 1\n",
      "1.3810435020597758 1\n",
      "1.735471978064644 2\n",
      "-0.017325612441437627 0\n",
      "-0.017325612441437627 0\n",
      "1.690176202902633 2\n",
      "1.3569730794265553 1\n",
      "1.2069916297154528 1\n",
      "1.2757818831859362 1\n",
      "-0.17078482901167555 0\n",
      "1.284906209755647 1\n",
      "1.05968350145411 1\n",
      "1.5578674620062043 1\n",
      "1.8077292056174743 2\n",
      "1.178911561198861 1\n",
      "\n",
      "\n",
      "##########\n",
      "\n",
      "\n",
      "\n",
      "Finished NM in 2 epochs with error 0.023202026048347882\n",
      "\n",
      "Optimal theta: [ 0.18649525 -0.11190585 -0.04007949  0.22864503  0.60925205]\n",
      "\n",
      "\n",
      "y_hat, y\n",
      "1.1198049997039012 1\n",
      "-0.0633428788940833 0\n",
      "0.8985638026579252 1\n",
      "0.9057464394012333 1\n",
      "1.3823647095058007 1\n",
      "-0.08986306757215701 0\n",
      "-0.10127251233909362 0\n",
      "1.2637618848515473 1\n",
      "0.07075023725288167 0\n",
      "1.4155283659004239 1\n",
      "0.01278319463890476 0\n",
      "1.2637618848515473 1\n",
      "1.7528952226625272 2\n",
      "1.4954411946302608 1\n",
      "-0.048627676790366614 0\n",
      "0.012299862700320574 0\n",
      "-0.15017620588129166 0\n",
      "0.0007593486904156066 0\n",
      "1.284824129064372 1\n",
      "0.038336719439810096 0\n",
      "-0.007040023312960442 0\n",
      "1.203946970535882 1\n",
      "1.742323706608306 2\n",
      "1.5614790840476118 2\n",
      "-0.09008192694628914 0\n",
      "0.038336719439810096 0\n",
      "-0.1251013448619398 0\n",
      "-0.02162415617370872 0\n",
      "-0.014572588673368969 0\n",
      "0.054149654663663505 0\n",
      "1.3009893269838413 1\n",
      "-0.08254936158588055 0\n",
      "0.9300609379411791 1\n",
      "0.058291006634643594 0\n",
      "-0.10506393569297323 0\n",
      "1.721042603021349 2\n",
      "1.6025889613272577 2\n",
      "-0.08973199832918866 0\n",
      "1.2883510798537996 1\n",
      "1.2024844167437596 1\n",
      "2.1543517490327595 2\n",
      "-0.041049498239638105 0\n",
      "1.241038675268177 1\n",
      "-0.04351966094221815 0\n",
      "1.7593221650834598 2\n",
      "1.1690562877846844 1\n",
      "0.8716890172057203 1\n",
      "1.4047458802914097 1\n",
      "1.058741722813215 1\n",
      "-0.08219709889026472 0\n",
      "1.7904693717496132 2\n",
      "1.1873713387541072 1\n",
      "1.1198049997039012 1\n",
      "1.742323706608306 2\n",
      "1.6006454096750664 2\n",
      "0.975923346711049 1\n",
      "1.9179166861015133 2\n",
      "-0.021271893478092885 0\n",
      "1.241038675268177 1\n",
      "-0.041049498239638105 0\n",
      "-0.1251013448619398 0\n",
      "0.019965831382212862 0\n",
      "0.07075023725288167 0\n",
      "-0.09008192694628914 0\n",
      "0.0007593486904156066 0\n",
      "1.1973810690526419 1\n",
      "2.0053644142702742 2\n",
      "-0.010783499398004714 0\n",
      "1.9001601992254686 2\n",
      "1.7904693717496132 2\n",
      "1.8574678103923796 2\n",
      "1.1854380109997704 1\n",
      "1.0520447520870064 1\n",
      "0.019965831382212862 0\n",
      "1.3416103165840791 1\n",
      "1.2039446364573667 1\n",
      "-0.155848341563642 0\n",
      "-0.08254936158588055 0\n",
      "1.4954411946302608 1\n",
      "-0.041049498239638105 0\n",
      "1.0520447520870064 1\n",
      "0.0007593486904156066 0\n",
      "1.058741722813215 1\n",
      "0.07075023725288167 0\n",
      "-0.021271893478092885 0\n",
      "1.0982547553954616 1\n",
      "1.382495778748769 1\n",
      "1.7314807157540866 2\n",
      "-0.010783499398004714 0\n",
      "-0.010783499398004714 0\n",
      "1.6930700844490074 2\n",
      "1.3472847863449449 1\n",
      "1.1854380109997704 1\n",
      "1.284824129064372 1\n",
      "-0.155848341563642 0\n",
      "1.2801994451548078 1\n",
      "1.0520447520870064 1\n",
      "1.5477384356404027 1\n",
      "1.7864646447624253 2\n",
      "1.2024844167437596 1\n",
      "\n",
      "\n",
      "\n",
      "GD time 0.20421648025512695 | NM time 0.0\n",
      "NM is faster\n"
     ]
    }
   ],
   "source": [
    "indices = [np.random.randint(0,m) for i in range(100)] #randomly sample 10 pairs for testing\n",
    "#print(theta)\n",
    "\n",
    "GD_time = gd(theta, y_hat, X_T, dist, cost, 0.032) # max 3 dp. cuz overflow\n",
    "print(\"\\n\\n\" + \"#\"*10 + \"\\n\\n\")\n",
    "NM_time = nm(theta, y_hat, X_T, dist, cost, 150)\n",
    "\n",
    "print(\"\\n\\n\\n\" + \"GD time\", GD_time, \"| NM time\", NM_time)\n",
    "if GD_time > NM_time:\n",
    "    print(\"NM is faster\")\n",
    "    #if runtime is to fast, can't always tell cuz of sig fig\n",
    "else:\n",
    "    print(\"GD is faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSANITY CHECK:\\n#foo1, foo2, foo3, foo4, foo5, foo6, foo7 = theta, y_hat, cost, Y, m, X, margin #insert before GD\\nassert(theta.all() == foo1.all())\\nassert(y_hat.all() == foo2.all())\\nassert(cost == foo3)\\nassert(Y.all() == foo4.all())\\nassert(m == foo5)\\nassert(X.all() == foo6.all())\\nassert(margin == foo7)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "SANITY CHECK:\n",
    "#foo1, foo2, foo3, foo4, foo5, foo6, foo7 = theta, y_hat, cost, Y, m, X, margin #insert before GD\n",
    "assert(theta.all() == foo1.all())\n",
    "assert(y_hat.all() == foo2.all())\n",
    "assert(cost == foo3)\n",
    "assert(Y.all() == foo4.all())\n",
    "assert(m == foo5)\n",
    "assert(X.all() == foo6.all())\n",
    "assert(margin == foo7)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
